# -*- coding: utf-8 -*-
"""IIP by SVR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ipZOoidkEYxT_X5XUbFD_NSYBsjBL60-
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

df = pd.read_csv('2004-2005final1csv.csv')
df

# @title Price of goods over time

import matplotlib.pyplot as plt
sns.lineplot(data=df, x="Period", y="Basic")
sns.lineplot(data=df, x="Period", y="Capital")
sns.lineplot(data=df, x="Period", y="Intermediate")
sns.lineplot(data=df, x="Period", y="Consumer total")
sns.lineplot(data=df, x="Period", y="Consumer Durable")
sns.lineplot(data=df, x="Period", y="Consumer Non-durable")
plt.title("Price of goods over time")
plt.xlabel("Period")
_ = plt.ylabel("Price")

df.info()

df.describe()

df.isnull().sum()

import yfinance as yf
import datetime as dt

continuous_cols = [col for col in df.columns if df[col].nunique()>15]
print(continuous_cols)

num_lst = []
cat_lst = []

from pandas.api.types import is_string_dtype, is_numeric_dtype

for column in df:
    plt.figure(column, figsize = (5,5))
    plt.title(column)
    if is_numeric_dtype(df[column]):
        df[column].plot(kind = 'hist')
        num_lst.append(column)
    elif is_string_dtype(df[column]):
        df[column].value_counts().plot(kind = 'bar')
        cat_lst.append(column)

print(num_lst)
print(cat_lst)

import os

print(df.isnull().sum().sum())

X = df.iloc[:, :-1].values #returns all rows and first column
y = df.iloc [:, -1].values #returns all rows and last column

from sklearn.linear_model import LinearRegression

from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score

specific_row = df.iloc[3]
X = df.iloc[1:,0].values.reshape(-1,1)
y = df.iloc[1:,4].values.reshape(-1,1)
from sklearn.model_selection import train_test_split
train_X,test_X,train_y,test_y=train_test_split(X,y,test_size=0.2,random_state=0)
poly_reg = PolynomialFeatures(degree=5)

X_poly = poly_reg.fit_transform(train_X)
X_poly_test = poly_reg.fit_transform(test_X)
model = LinearRegression()

model.fit(X_poly,train_y)
m1 = model.coef_
print("y = {}*Period + {}".format(m1[0][1],m1[0][0]))

# Predict the test set results
y_pred = model.predict(X_poly_test)
mse = mean_squared_error(test_y, y_pred)
r2 = r2_score(test_y, y_pred)
print(r2)
print(model.coef_)

plt.figure(figsize=(10,10))
X = df.iloc[1:,0].values
y = df.iloc[1:,2].values
plt.scatter(X,y)
plt.show()

from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score

print("MSE : ",mean_squared_error(y_pred,test_y))
print("MAE : ",mean_absolute_error(y_pred,test_y))

print(y)

X = df.iloc[1:,0].values.reshape(-1,1)
y = df.iloc[1:,2].values.reshape(-1,1)

y = y.reshape(len(y),1)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X = sc_X.fit_transform(X)
y = sc_y.fit_transform(y)


# Training the SVR model on the whole dataset
from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf')
regressor.fit(X, y)

# Visualising the SVR results (for higher resolution and smoother curve)
X_grid = np.arange(min(sc_X.inverse_transform(X)), max(sc_X.inverse_transform(X)), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'red')
plt.plot(X_grid, sc_y.inverse_transform(regressor.predict(sc_X.transform(X_grid)).reshape(-1,1)), color = 'blue')
plt.xlabel('Months yearwise')
plt.ylabel('IIP of basic goods')
plt.show()



plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'red')
plt.plot(sc_X.inverse_transform(X), sc_y.inverse_transform(regressor.predict(X).reshape(-1,1)), color = 'blue')
plt.title('SVR')
plt.xlabel('Months yearwise')
plt.ylabel('IIP of basic goods')
plt.show()


# Predicting a new result
sc_y.inverse_transform(regressor.predict(sc_X.transform([[145]])).reshape(-1,1))

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

# Commented out IPython magic to ensure Python compatibility.
import tensorflow.compat.v1 as tf
import numpy as np
import warnings
import matplotlib.pyplot as plt
# %matplotlib inline

tf.disable_eager_execution()
warnings.filterwarnings('ignore')

#run time step
time_step = 10
#input size
input_size = 1
#run cell size
cell_size = 32
# learning rate
learning_rate = 0.02

tf_x = tf.placeholder(tf.float32, [None, time_step, input_size])
tf_y = tf.placeholder(tf.float32, [None, time_step, input_size])

rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units = cell_size)
init_s = rnn_cell.zero_state(batch_size = 1, dtype = tf.float32)

outputs, final_s = tf.nn.dynamic_rnn(rnn_cell,
                                    tf_x,                      # Input
                                    initial_state= init_s,      # The initial hidden Layers
                                    time_major= False)

outs_2D= tf.reshape(outputs, [-1, cell_size])
net_outs2D = tf.layers.dense(outs_2D, input_size)
outs = tf.reshape(net_outs2D, [-1, time_step, input_size])

loss = tf.losses.mean_squared_error(labels = tf_y, predictions=outs)
train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler()

"""White Noise Test"""

from pandas.plotting import autocorrelation_plot
import matplotlib.pyplot as plt
import numpy as np
randval = np.random.randn(1000)
autocorrelation_plot(randval)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set("talk","ticks",font_scale=1,font="Calibri")
from pylab import rcParams
plt.rcParams["figure.dpi"] = 300
import warnings
warnings.filterwarnings("ignore")
import statsmodels.api as sm
import itertools
from pandas.plotting import lag_plot, autocorrelation_plot
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose

df_basic = pd.DataFrame(df['Basic'])

df_basic.head()

def create_regressor_attributes(df, attribute, list_of_prev_t_instants) :
    list_of_prev_t_instants.sort()
    start = list_of_prev_t_instants[-1]
    end = len(df)
    df['datetime'] = df.index
    df.reset_index(drop=True)

    df_copy = df[start:end]
    df_copy.reset_index(inplace=True, drop=True)

    for attribute in attribute :
            foobar = pd.DataFrame()

            for prev_t in list_of_prev_t_instants :
                new_col = pd.DataFrame(df[attribute].iloc[(start - prev_t) : (end - prev_t)])
                new_col.reset_index(drop=True, inplace=True)
                new_col.rename(columns={attribute : '{}_(t-{})'.format(attribute, prev_t)}, inplace=True)
                foobar = pd.concat([foobar, new_col], sort=False, axis=1)

            df_copy = pd.concat([df_copy, foobar], sort=False, axis=1)

    df_copy.set_index(['datetime'], drop=True, inplace=True)
    return df_copy

list_of_attributes = ['Basic']

list_of_prev_t_instants = []
for i in range(1,16):
    list_of_prev_t_instants.append(i)

list_of_prev_t_instants

df_new = create_regressor_attributes(df_basic, list_of_attributes, list_of_prev_t_instants)
df_new.head()

df_new

df_new.shape

from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.models import Model
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import ModelCheckpoint

input_layer = Input(shape=(15), dtype='float32')
dense1 = Dense(60, activation='linear')(input_layer)
dense2 = Dense(60, activation='linear')(dense1)
dropout_layer = Dropout(0.2)(dense2)
output_layer = Dense(1, activation='linear')(dropout_layer)

model = Model(inputs=input_layer, outputs=output_layer)
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

test_set_size = 0.05
valid_set_size= 0.05

df_copy = df_new.reset_index(drop=True)

df_test = df_copy.iloc[ int(np.floor(len(df_copy)*(1-test_set_size))) : ]
df_train_plus_valid = df_copy.iloc[ : int(np.floor(len(df_copy)*(1-test_set_size))) ]

df_train = df_train_plus_valid.iloc[ : int(np.floor(len(df_train_plus_valid)*(1-valid_set_size))) ]
df_valid = df_train_plus_valid.iloc[ int(np.floor(len(df_train_plus_valid)*(1-valid_set_size))) : ]


X_train, y_train = df_train.iloc[1:, 3:], df_train.iloc[:, 0]
X_valid, y_valid = df_valid.iloc[1:, 3:], df_valid.iloc[:, 0]
X_test, y_test = df_test.iloc[1:, 3:], df_test.iloc[:, 0]

print('Shape of training inputs, training target:', X_train.shape, y_train.shape)
print('Shape of validation inputs, validation target:', X_valid.shape, y_valid.shape)
print('Shape of test inputs, test target:', X_test.shape, y_test.shape)

print(X_train)

from sklearn.preprocessing import MinMaxScaler

Target_scaler = MinMaxScaler(feature_range=(0.01, 0.99))
Feature_scaler = MinMaxScaler(feature_range=(0.01, 0.99))

X_train_scaled = Feature_scaler.fit_transform(np.array(X_train))
X_valid_scaled = Feature_scaler.fit_transform(np.array(X_valid))
X_test_scaled = Feature_scaler.fit_transform(np.array(X_test))

y_train_scaled = Target_scaler.fit_transform(np.array(y_train).reshape(-1,1))
y_valid_scaled = Target_scaler.fit_transform(np.array(y_valid).reshape(-1,1))
y_test_scaled = Target_scaler.fit_transform(np.array(y_test).reshape(-1,1))

model.fit(X_train, y_train, epochs=30)

from keras.layers import LSTM

model.compile(loss='mse', optimizer='adam')

model.fit(X_train, y_train, epochs=30)

y_pred_rescaled = Target_scaler.inverse_transform(y_pred)
pd.DataFrame(y_pred_rescaled, columns = ["Forecast"]).head()

from sklearn.metrics import r2_score
y_test_rescaled =  Target_scaler.inverse_transform(y_test_scaled)
score = r2_score(y_test_rescaled, y_pred_rescaled)
print('R-squared score for the test set:', round(score,4))

from sklearn import metrics

MAE = metrics.mean_absolute_error(y_test_rescaled,y_pred_rescaled)
MSE = metrics.mean_squared_error(y_test_rescaled,y_pred_rescaled)
RMSE = np.sqrt(MSE)
R2 = metrics.r2_score(y_test_rescaled,y_pred_rescaled)
EV = metrics.explained_variance_score(y_test_rescaled,y_pred_rescaled)
MGD = metrics.mean_gamma_deviance(y_test_rescaled,y_pred_rescaled)
MPD = metrics.mean_poisson_deviance(y_test_rescaled,y_pred_rescaled)
lmmodelevaluation = [[MAE,MSE,RMSE,R2,EV,MGD,MPD]]
lmmodelevaluationdata = pd.DataFrame(lmmodelevaluation,
                                     index = ["Values"],
                                     columns = ["MAE",
                                                "MSE",
                                                "RMSE",
                                                "R2",
                                                "Explained variance score",
                                                "Mean gamma deviance",
                                                "Mean Poisson deviance"]).transpose()
lmmodelevaluationdata

y_actual = pd.DataFrame(y_test_rescaled, columns=['Actual Close Price'])
y_hat = pd.DataFrame(y_pred_rescaled, columns=['Predicted Close Price'])

plt.plot(y_actual,color='red')
plt.plot(y_hat, linestyle='dashed', color='navy')
plt.legend(['Actual','Predicted'], loc='best')
plt.ylabel('  IIP')
plt.xlabel('Test Set Day no.')
plt.xticks(rotation=45)
plt.yticks()
plt.show()

plt.plot(history.history["loss"],color="red",label="Training Loss")
plt.plot(history.history["val_loss"],color="navy",label="Cross-Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(loc="best")
plt.show()

from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Prepare the data
df = pd.read_csv('2004-2005final1csv.csv')
data = df.filter(['Basic']).values
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# Split the data into train and test sets
train_size = int(len(scaled_data) * 0.8)
X_train

history = model.fit(x=X_train_scaled, y=y_train_scaled, batch_size=5, epochs=30, verbose=1, validation_data=(X_valid_scaled, y_valid_scaled), shuffle=True)

plt.plot(df)